This is the README file for A0121437N's submission

== General Notes about this assignment ==

index.py contains the functions used to build the index. These functions are
build_index, write_index and generate_postings_string. The build_index function
reads each individual file in the corpus and splits them into word tokens using
the nltk word_tokenizer method. This function returns a python dictionary with
keys being the word tokens and the values being the lists of document id and
normalized log-weighted term frequency tuples. This dictionary is then passed to
the write_index function which iterates over they keys of the dictionary and
generates a 'postings string', written to the postings output file.

I chose to calculate the normalize log-weighted term frequencies for each term
in each document during this indexing phase as opposed to during the querying
phase as it would increase the querying efficiency. This, however, is at the
cost of the size of the postings file as now instead of storing integers for
term frequencies I have to store floats. To save a bit of space, instead of
writing the entire float to the postings file, I first remove the first 2
characters of the float, i.e. the '0.'. This decreased the postings file size by
roughly 1.2MB, or about 10% of the size of the postings file with '0.'s.

To generate the string for a postings list, the generate_postings_string
function is called which converts an ordered list of document ids and normalized
log-weighted term frequecies into a string. The following example explains the
format of the output string.

Assume we have a postings list with three documents and their corresponding
normalized log-weighted term frequecies, [(1, 0.123), (2, 0.456), (3, 0.789)].
The string that is output that is to be written to the file is:
"1 123 2 456 3 789\n". Notice that the '0.' before each of the normalized log-
weighted term frequecies have been removed as explained in the previous
paragraph as a space saving measure.

The write_index function writes each of the strings generated by the
generate_postings_string function for each of the postings lists for each of the
tokens in the dictionary created by the build_index function. It also writes the
term, a pointer to the starting byte of the the location of the terms postings
list, and the inverse document frequency into the dictionary file. I calculate
the idf of each document during this indexing phase for the same reason as I do
for the term frequencies, to make queries more efficient, at the cost of
dictionary size.

search.py contains the functions used to find the relevant documents that match
the free text queries. First, build_dict builds a python dictionary based on the
dictionary file output by index.py. This dictionary's keys are the stemmed
tokens and the values are tuples of the pointer to the location of the start of
the postings list and the idf of the token.

This dictionary is then passed to execute_queries to execute each of the queries
found in the input query file. Each of the lines in the query file are passed to
process_query to find the most relevant documents.

process_query does this by first calling normalize_query_term_frequencies. This
function splits the query into tokens using nltk's word_tokenize function, and
calculates the tf-idf for each of these query tokens. A dictionary of the tokens
and their respective tf-idf values is returned.

Next, process_query calls get_document_normalized_term_freq, which gets the
normalized term frequencies for each document containing the tokens provided in
the query. Since the term frequencies are already log-weighted and normalized in
indexing stage, this function simply retrieves their value using PostingReader.
It then returns a dictionary of dictionaries. The outer dictionary is keyed by
the doc_ids of the documents containing the tokens, and the inner dictionary is
keyed by the tokens present in the document, with values being the normalized
term frequencies of that term in that document.

PostingReader provides a layer of abstraction over python's file seek and read
functions. PostingReader is initialized using a python file object of the
postings file and the pointer to the start of a postings list for a token. The
document ids and term frequencies are then retrieved by calling the
PostingReader's next method.

Lastly, the output of the of the two functions, normalize_query_term_frequencies
and get_document_normalized_term_freq, are passed to score_documents which
calculates the cosine score of each document. process_query then returns the
output of score_documents, which is a list of document ids, ordered by their
cosine scores in decending order.

== Files included with this submission ==

index.py:       Creates the dictionary and postings from the supplied corpus.
search.py:      Takes in a query file and writes the output to a specified
                output file.
dictionary.txt: The dictionary file index.py produced when run on my machine.
postings.txt:   The postings file index.py produced when run on my machine.

== Statement of individual work ==

Please initial one of the following statements.

[X] I, A0121437N, certify that I have followed the CS 3245 Information
Retrieval class guidelines for homework assignments.  In particular, I
expressly vow that I have followed the Facebook rule in discussing
with others in doing the assignment and did not take notes (digital or
printed) from the discussions.  

[ ] I, A0121437N, did not follow the class rules regarding homework
assignment, because of the following reason:

<Please fill in>

I suggest that I should be graded as follows:

<Please fill in>

== References ==

<Please list any websites and/or people you consulted with for this
assignment and state their role>
