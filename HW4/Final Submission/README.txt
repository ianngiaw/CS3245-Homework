This is the README file for A0121437N-A0122081X's submission

== General Notes about this assignment ==

# ========================================
# Indexing
# ========================================

## Doc ID to IPC ##
## Term to IPC Class ##
## Term to IPC Subclass ##
## Term to IPC Group ##

index.py contains the functions used to build the index. These functions are
build_index, write_index and generate_postings_string. The build_index function
reads each individual file in the corpus and splits them into word tokens using
the nltk word_tokenizer method. This function returns a python dictionary with
keys being the word tokens and the values being the lists of document id and
normalized log-weighted term frequency tuples. This dictionary is then passed to
the write_index function which iterates over they keys of the dictionary and
generates a 'postings string', written to the postings output file.

I chose to calculate the normalize log-weighted term frequencies for each term
in each document during this indexing phase as opposed to during the querying
phase as it would increase the querying efficiency. This, however, is at the
cost of the size of the postings file as now instead of storing integers for
term frequencies I have to store floats. To save a bit of space, instead of
writing the entire float to the postings file, I first remove the first 2
characters of the float, i.e. the '0.'. This decreased the postings file size by
roughly 1.2MB, or about 10% of the size of the postings file with '0.'s.

To generate the string for a postings list, the generate_postings_string
function is called which converts an ordered list of document ids and normalized
log-weighted term frequecies into a string. The following example explains the
format of the output string.

Assume we have a postings list with three documents and their corresponding
normalized log-weighted term frequecies, [(1, 0.123), (2, 0.456), (3, 0.789)].
The string that is output that is to be written to the file is:
"1 123 2 456 3 789\n". Notice that the '0.' before each of the normalized log-
weighted term frequecies have been removed as explained in the previous
paragraph as a space saving measure.

The write_index function writes each of the strings generated by the
generate_postings_string function for each of the postings lists for each of the
tokens in the dictionary created by the build_index function. It also writes the
term, a pointer to the starting byte of the the location of the terms postings
list, and the inverse document frequency into the dictionary file. I calculate
the idf of each document during this indexing phase for the same reason as I do
for the term frequencies, to make queries more efficient, at the cost of
dictionary size.



# ========================================
# Search
# ========================================

search.py contains the functions used to find the relevant documents that match
the free text queries. First, build_dict builds a python dictionary based on the
dictionary file output by index.py. This dictionary's keys are the stemmed
tokens and the values are tuples of the pointer to the location of the start of
the postings list and the idf of the token.

This dictionary is then passed to execute_queries to execute each of the queries
found in the input query file. Each of the lines in the query file are passed to
process_query to find the most relevant documents.

process_query does this by first calling normalize_query_term_frequencies. This
function splits the query into tokens using nltk's word_tokenize function, and
calculates the tf-idf for each of these query tokens. A dictionary of the tokens
and their respective tf-idf values is returned.

Next, process_query calls get_document_normalized_term_freq, which gets the
normalized term frequencies for each document containing the tokens provided in
the query. Since the term frequencies are already log-weighted and normalized in
indexing stage, this function simply retrieves their value using PostingReader.
It then returns a dictionary of dictionaries. The outer dictionary is keyed by
the doc_ids of the documents containing the tokens, and the inner dictionary is
keyed by the tokens present in the document, with values being the normalized
term frequencies of that term in that document.

PostingReader provides a layer of abstraction over python's file seek and read
functions. PostingReader is initialized using a python file object of the
postings file and the pointer to the start of a postings list for a token. The
document ids and term frequencies are then retrieved by calling the
PostingReader's next method.

Lastly, the output of the of the two functions, normalize_query_term_frequencies
and get_document_normalized_term_freq, are passed to score_documents which
calculates the cosine score of each document. process_query then returns the
output of score_documents, which is a list of document ids, ordered by their
cosine scores in decending order.


# ========================================
# Originality of ideas
# ========================================

In this patent retrieval project, we made use of IPC Class, IPC Subclass and IPC Group, which will henceforth be referred to collectively as IPC categories. We treated each member each of these categories as a document and indexed them exactly the same way as we did with the patent documents, except now in place of document ids, we have IPC classes/subclasses/groups as the identifier.

For example, if a term "liquid" appears in a document with IPC Class "H05", "C12" and "G05", the postings will look like "liquid": "H05" -> "C12" -> "G05".

For our final submission, we used a combination of IPC and VSM model without Rocchio, because that was the combination that gave us the highest MAF2 scores. The overview of the system and its architecture will be discussed in the next segment, while the remaining of this segment will be dedicated to the other original ideas that we have tried.

We tried the following combinations:
1. Language Model (LM)
2. LM + VSM with Rocchio

For the pure LM, the documents retrieved were simply a results of running the LM. However, the results we got were not ideal and resulted in a low F2 score, so we tried used both LM and VSM to retrieve the documents and combined them in to a large set of retrieved documents. However, by combining the LM results with the VSM results, the overall F2 score was actually lowered, hence we decided to ditch the LM altogether.

After deciding to use only VSM, we also tried experimenting with the heuristics we used to decide Relevant and Non-Relevant documents for the Rocchio implementation:
1. Non-relevant set of documents as the entire corpus except the top k documents
2. Non-relevant set of documents as non-retrieved documents

For method 1, we tried using top 10, 50 and 100 and using the top 100 gave the best results. However, method 2 performed slightly better than method 1, so we used that heuristics instead. However, our final implementation did not include Rocchio at all.

Eventually, after finding out that IPC + VSM model gave us the highest MAF2 values, we tried tweaking the parameters in this approach:
1. Identical weights for all IPC categories (numerator is squared during normalisation)
2. Identical weights for all IPC categories (numerator is calculated normally during normalisation)
3. Different weights for each IPC categories. In order of decreasing weight: IPC group, IPC subclass, IPC class

Method 1 was a result of a bug in our programming, which resulted in the numerator being squared when the normalised IPC score was being calculated. 

Method 2 was our attempt to fix the mistake committed in method 1. 

For method 3, we weighted IPC group higher because we felt that it is a better differentiator than both subclass and class, since it is more specific. 

We tried all three methods for evalution and method 1 returned the highest score by a large margin. Going by this logic, we thought that by increasing the factor of the exponent, the F2 score might even be higher. We tried a factor of 3 initially, but the score dropped. Increasing to a factor of 2.5 however raised the scores, so our final implementation used a factor of 2.5.

# =================================================
# Overview of the system and system architecture
# ==================================================

indexing step indexes all the ipc categories
normal term -> doc postings list
pure vsm lel

1. How your system deals with each of the optional components (query expansion, utilizing external resources, field/zone treatment
query expansion not included in the final submission (commented out), since it performed worse when used at all (with and without ipc). describe how we did query expansion
no external resources.
query expansion with rocchio we used family and cites fields, buut didn't do as well so we commented it out for search

2. Fields that we took note of ipc categories 
Zones that we took note of title and abstract -> concatenated and treated as the one single document. did not treat them as from different zones

3. Run-time optimizations
not using internet resources as a run-time optimisation LOL
tf and idf both calculated during indexing phase and stored inside dictionary and postings file, rather than calculating during query time. save time during query, no need to perform the arithmetic operations.

4. Allocation of work to each of the individual members of the project team.
Ian did all of the indexing, VSM model, implementation of the IPC implementation, code documentation
MX did the language model, research on how to improve the MAF score and writing up the readme

== Files included with this submission ==

index.py:       Creates the dictionary and postings from the supplied corpus.
search.py:      Takes in a query file and writes the output to a specified
                output file.
dictionary.txt: The dictionary file index.py produced when run on my machine.
postings.txt:   The postings file index.py produced when run on my machine.

== Statement of individual work ==

Please initial one of the following statements.

[X] I, A0121437N-A0122081X, certify that I have followed the CS 3245 Information
Retrieval class guidelines for homework assignments.  In particular, I
expressly vow that I have followed the Facebook rule in discussing
with others in doing the assignment and did not take notes (digital or
printed) from the discussions.  

[ ] I, A0121437N-A0122081X, did not follow the class rules regarding homework
assignment, because of the following reason:

<Please fill in>

I suggest that I should be graded as follows:

<Please fill in>

== References ==

<Please list any websites and/or people you consulted with for this
assignment and state their role>
